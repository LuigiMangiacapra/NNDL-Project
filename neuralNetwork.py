import numpy as np
import activationFunctions as af

class NeuralNetwork:
    MU, SIGMA = 0, 0.1

    # hidden_activation_functions: list of activation functions for each hidden layer
    # output_activation_function: activation function for the output layer
    # error_function: error function to be used
    # input_layer_size: number of input neurons
    # hidden_layer_size: number of neurons in each hidden layer
    # output_layer_size: number of neurons in the output layer
    # number_of_hidden_layers: number of hidden layers
    def __init__(self, hidden_activation_functions, output_activation_function, error_function,
                input_layer_size, hidden_layers, output_layer_size):

        self.layers_bias = []
        self.layers_weights = []
        self.hidden_layer_size = hidden_layers

        if len(hidden_activation_functions) != len(hidden_layers):
            raise ValueError("The number of hidden activation function must be equal to the number of hidden layers")
        
        self.hidden_activation_functions = hidden_activation_functions
        self.hidden_activation_functions = hidden_activation_functions
        
        hidden_activation_functions.append(output_activation_function)
        
        #self.output_activation_function = output_activation_function
        self.error_function = error_function
        
        self.number_of_hidden_layers = len(hidden_layers)
    
        self.__initialize_parameters(input_layer_size, output_layer_size)

    def __initialize_parameters(self, input_layer_size, output_layer_size):
        hidden_layer_size = self.hidden_layer_size
        number_of_hidden_layers = self.number_of_hidden_layers

        self.__initialize_weights(0, hidden_layer_size[0], input_layer_size)
        self.__initialize_bias(0, hidden_layer_size[0])

        for i in range(1, self.number_of_hidden_layers):
            self.__initialize_weights(i, hidden_layer_size[i], hidden_layer_size[i])
            self.__initialize_bias(i, hidden_layer_size[i])

        self.__initialize_weights(number_of_hidden_layers, output_layer_size, hidden_layer_size[number_of_hidden_layers - 1])
        self.__initialize_bias(number_of_hidden_layers, output_layer_size)

    def __initialize_weights(self, index, number_of_layer_neurons, input_variables):
        # the number are generated by the standard distribution (gaussian)
        self.layers_weights.insert(index, np.random.normal(self.MU, self.SIGMA, size=(number_of_layer_neurons,
                                                                                      input_variables)))
        
    def __initialize_bias(self, index, number_of_layer_neurons):
        self.layers_bias.insert(index, np.random.normal(self.MU, self.SIGMA, size=(number_of_layer_neurons, 1)))


    def get_weights(network,i=0):
        W=network.layers_weights
        if (i>0):
            return W[i-1]
        else:
            return W

    def get_biases(network,i=0):
        B=network.layers_bias
        if (i>0):
            return B[i-1]
        else:
            return B

    def get_act_fun(network,i=0):
        AF=network.hidden_activation_functions
        if (i>0):
            return AF[i-1]
        else:
            return AF
        
    def set_weights(network, layer_indices, weight_matrices, bias=0):
        
        count = 0
        # Set weights or biases for multiple layers
        if bias == 0:
            for i in layer_indices:
                network.layers_weights[i - 1] = weight_matrices[count].copy()
                count += 1
        else:
            for i in layer_indices:
                network.layers_bias[i - 1] = weight_matrices[count].copy()
                count += 1

        return network
    
    def set_activation_function(network, layer_indices=[], activation_function=af.tanh, layer_type=1):

        if layer_type == 0:  # Set activation function for specific layer(s)
            if np.isscalar(layer_indices):
                network.hidden_activation_functions[layer_indices - 1] = activation_function
            else:
                count = 0
                for i in layer_indices:
                    network.hidden_activation_functions[i - 1] = activation_function[count]
                    count += 1
        elif layer_type == 1:  # Set activation function for all hidden layers
            for i in range(network.number_of_hidden_layers - 1):  # Exclude the output layer
                network.hidden_activation_functions[i] = activation_function
        else:  # Set activation function for the output layer
            network.output_activation_function = activation_function

        return network

